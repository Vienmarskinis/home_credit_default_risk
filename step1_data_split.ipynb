{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Introduction and Data Splitting\n",
    "## Goals in this Notebook\n",
    "To introduce the reader with the project and create a path for the project to be executed. Then, split the data at hand into train and test.\n",
    "## Introduction\n",
    "TODO\n",
    "## Project Plan\n",
    "This plan will be iteratively modified to reflect new findings in the dataset and potential improvements in the proof-of-concept.\n",
    "### Project Structure\n",
    "**Step 1. Test / Train data split**\n",
    "- The first step is to split the data to have sufficient data for testing the models which are the ultimate goal of this project.\n",
    "\n",
    "**Step 2. EDA**\n",
    "- Get acquainted with the data at hand. Not all of this process will be reflected in the notebooks due to the vastness of features, but a part of it will. I will start with univariate analysis, checking what data and at what proportions is present in the various tables.\n",
    "\n",
    "**Step 3. Identify a potential business problem**\n",
    "- During EDA, a potential problem should emerge.\n",
    "\n",
    "**Step 4. Prepare data for the problem**\n",
    "- Find out what data could be used to train a model to solve the identified problem.\n",
    "\n",
    "**Step 5. Train and select the best model to solve the problem**\n",
    "\n",
    "**(Go to Step 3 until several potential problems are solved)**\n",
    "\n",
    "**Step 6. Test and deploy the selected models**\n",
    "### Assumptions\n",
    "- We are most interested about the customer credit history which happened during the last year. That being said, we will still use data from the whole history at hand.\n",
    "- TARGET feature is defined as \"client with payment difficulties: he/she had late payment more than X days on at least one of the first Y installments of the loan in our sample\". In this project I will not try to guess the X and Y but rather try to create features that could define a client with potential payment difficulties.\n",
    "### Potential Business Problems\n",
    "- Predict the \"TARGET\" feature in the application_train table. This feature indicates whether a client has payment problems.\n",
    "- Predict the \"NAME_CONTRACT_STATUS\" in the previous_application table. This feature indicates whether the application was approved, cancelled, refused.\n",
    "- Predict the \"CODE_REJECT_REASON\" in the previous_application table. This feature indicates why the application was rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-in Data, Convert to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_app = r\"data\\application_train.csv\"\n",
    "path_bur = r\"data\\bureau.csv\"\n",
    "path_bur_bal = r\"data\\bureau_balance.csv\"\n",
    "path_prev_app = r\"data\\previous_application.csv\"\n",
    "path_cash = r\"data\\POS_CASH_balance.csv\"\n",
    "path_inst = r\"data\\installments_payments.csv\"\n",
    "path_cred = r\"data\\credit_card_balance.csv\"\n",
    "\n",
    "df_app = functions.csv_to_parquet(path_app)\n",
    "df_bur = functions.csv_to_parquet(path_bur)\n",
    "df_bur_bal = functions.csv_to_parquet(path_bur_bal)\n",
    "df_prev_app = functions.csv_to_parquet(path_prev_app)\n",
    "df_cash = functions.csv_to_parquet(path_cash)\n",
    "df_inst = functions.csv_to_parquet(path_inst)\n",
    "df_cred = functions.csv_to_parquet(path_cred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    307511\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    1716428\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bur.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    27299925\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bur_bal.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    1670214\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prev_app.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    10001358\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cash.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3840312\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cred.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    13605401\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inst.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split df_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since df_app sits at the top of the hierarchy of all the tables, I would like to split it according to this table.\n",
    "\n",
    "One thing that could be improved here: the application_test.csv has no \"Target\" column, but the data could still potentially be merged in and used for some other purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app_train, df_app_test = train_test_split(\n",
    "    df_app, test_size=0.05, shuffle=True, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Train Split on Other Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other tables will be split based on df_app ids. Bureau is easy to split. Cash, inst and cred dataframes are harder, since they can be handled directly by SK_ID_CURR or through df_app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_ids = df_app_train[\"SK_ID_CURR\"]\n",
    "\n",
    "# Handle Bureau\n",
    "df_bur_train = df_bur[df_bur[\"SK_ID_CURR\"].isin(curr_ids)]\n",
    "bureau_ids = df_bur_train[\"SK_ID_BUREAU\"]\n",
    "df_bur_bal_train = df_bur_bal[df_bur_bal[\"SK_ID_BUREAU\"].isin(bureau_ids)]\n",
    "\n",
    "# Handle cash, inst, cred tables directly through SK_ID_CURR\n",
    "df_cash_curr = df_cash[df_cash[\"SK_ID_CURR\"].isin(curr_ids)]\n",
    "df_inst_curr = df_inst[df_inst[\"SK_ID_CURR\"].isin(curr_ids)]\n",
    "df_cred_curr = df_cred[df_cred[\"SK_ID_CURR\"].isin(curr_ids)]\n",
    "\n",
    "# Handle cash, inst, cred tables through prev_app table\n",
    "df_prev_app_train = df_prev_app[df_prev_app[\"SK_ID_CURR\"].isin(curr_ids)]\n",
    "prev_ids = df_prev_app_train[\"SK_ID_PREV\"]\n",
    "df_cash_prev = df_cash[df_cash[\"SK_ID_PREV\"].isin(prev_ids)]\n",
    "df_inst_prev = df_inst[df_inst[\"SK_ID_PREV\"].isin(prev_ids)]\n",
    "df_cred_prev = df_cred[df_cred[\"SK_ID_PREV\"].isin(prev_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cash table when handled directly: (8117603, 8)\n",
      "Shape of cash table when handled indirectly: (7841360, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of cash table when handled directly:\", df_cash_curr.shape)\n",
    "print(\"Shape of cash table when handled indirectly:\", df_cash_prev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When handled indirectly, we get a smaller dataset, which would imply that we don't have all the previous applications in the prev_app table.\n",
    "\n",
    "We can also check if the \"_curr\" tables are supersets of the \"_prev\" tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cash_curr table is a superset of cash_prev\n",
      "inst_curr table is a superset of inst_prev\n",
      "cred_curr table is a superset of cred_prev\n"
     ]
    }
   ],
   "source": [
    "cols = [\"SK_ID_CURR\", \"SK_ID_PREV\"]\n",
    "functions.check_if_superset(df_cash_curr, df_cash_prev, cols, \"cash_curr\", \"cash_prev\")\n",
    "functions.check_if_superset(df_inst_curr, df_inst_prev, cols, \"inst_curr\", \"inst_prev\")\n",
    "functions.check_if_superset(df_cred_curr, df_cred_prev, cols, \"cred_curr\", \"cred_prev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the \"curr\" tables are supersets, we'll only save them and discard \"prev\" tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cash_train = df_cash_curr.copy()\n",
    "df_inst_train = df_inst_curr.copy()\n",
    "df_cred_train = df_cred_curr.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Test Split on Other Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also define the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_ids = df_app_test[\"SK_ID_CURR\"]\n",
    "\n",
    "# Handle Bureau\n",
    "df_bur_test = df_bur[df_bur[\"SK_ID_CURR\"].isin(curr_ids)]\n",
    "bureau_ids = df_bur_test[\"SK_ID_BUREAU\"]\n",
    "df_bur_bal_test = df_bur_bal[df_bur_bal[\"SK_ID_BUREAU\"].isin(bureau_ids)]\n",
    "\n",
    "# Handle cash, inst, cred tables directly through SK_ID_CURR\n",
    "df_cash_test = df_cash[df_cash[\"SK_ID_CURR\"].isin(curr_ids)]\n",
    "df_inst_test = df_inst[df_inst[\"SK_ID_CURR\"].isin(curr_ids)]\n",
    "df_cred_test = df_cred[df_cred[\"SK_ID_CURR\"].isin(curr_ids)]\n",
    "\n",
    "# Handle prev_app\n",
    "df_prev_app_test = df_prev_app[df_prev_app[\"SK_ID_CURR\"].isin(curr_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app_train.to_parquet(r\"data\\train\\df_app.parquet\", engine=\"pyarrow\")\n",
    "df_app_test.to_parquet(r\"data\\test\\df_app.parquet\", engine=\"pyarrow\")\n",
    "df_bur_train.to_parquet(r\"data\\train\\df_bur.parquet\", engine=\"pyarrow\")\n",
    "df_bur_test.to_parquet(r\"data\\test\\df_bur.parquet\", engine=\"pyarrow\")\n",
    "df_bur_bal_train.to_parquet(r\"data\\train\\df_bur_bal.parquet\", engine=\"pyarrow\")\n",
    "df_bur_bal_test.to_parquet(r\"data\\test\\df_bur_bal.parquet\", engine=\"pyarrow\")\n",
    "df_prev_app_train.to_parquet(r\"data\\train\\df_prev_app.parquet\", engine=\"pyarrow\")\n",
    "df_prev_app_test.to_parquet(r\"data\\test\\df_prev_app.parquet\", engine=\"pyarrow\")\n",
    "df_cash_train.to_parquet(r\"data\\train\\df_cash.parquet\", engine=\"pyarrow\")\n",
    "df_cash_test.to_parquet(r\"data\\test\\df_cash.parquet\", engine=\"pyarrow\")\n",
    "df_inst_train.to_parquet(r\"data\\train\\df_inst.parquet\", engine=\"pyarrow\")\n",
    "df_inst_test.to_parquet(r\"data\\test\\df_inst.parquet\", engine=\"pyarrow\")\n",
    "df_cred_train.to_parquet(r\"data\\train\\df_cred.parquet\", engine=\"pyarrow\")\n",
    "df_cred_test.to_parquet(r\"data\\test\\df_cred.parquet\", engine=\"pyarrow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
